import os
import torch
import streamlit as st
import numpy as np
import cv2
from PIL import Image
from groundingdino.util.inference import load_model, load_image, predict, annotate

# í™˜ê²½ ì„¤ì •
os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "1"

# HOME ê²½ë¡œ ì„¤ì •
HOME = os.path.expanduser("~")

CONFIG_PATH = "./Grounding_Dino_Auto_Annotaion/test/GroundingDINO/groundingdino/config/GroundingDINO_SwinB_cfg.py"
WEIGHTS_PATH = "./Grounding_Dino_Auto_Annotaion/test/GroundingDINO/weights/groundingdino_swinb_cogcoor.pth"


device = "mps" if torch.backends.mps.is_available() else "cpu"

# Grounding DINO ëª¨ë¸ ë¡œë“œ
model = load_model(CONFIG_PATH, WEIGHTS_PATH).to(device)

# Streamlit UI êµ¬ì„±
st.set_page_config(layout="wide", page_title="Grounding DINO Auto Labeling")

st.title("ğŸ¦– Grounding DINO Auto Labeling Tool")

# íŒŒì¼ ì—…ë¡œë“œ ê¸°ëŠ¥
uploaded_file = st.file_uploader("Upload an image", type=["jpg", "jpeg", "png"])

# ê¸°ë³¸ í´ë˜ìŠ¤ ì„¤ì •
class_labels_input = st.sidebar.text_area("âœï¸ Enter object classes (comma-separated)", "")
class_labels = [c.strip() for c in class_labels_input.split(",") if c.strip()]


# Confidence Threshold ìŠ¬ë¼ì´ë” ì„¤ì •
threshold_values = {}
if class_labels:
    for class_name in class_labels:
        threshold_values[class_name] = st.sidebar.slider(
            f"ğŸ” {class_name} Confidence Threshold",
            min_value=0.1, max_value=0.95, value=0.5, step=0.05
        )

apply_detection = st.sidebar.button("Apply")


if uploaded_file is not None:
    # ì´ë¯¸ì§€ ë¡œë“œ
    image = Image.open(uploaded_file).convert("RGB")
    image_array = np.array(image)
    
    st.image(image_array, caption="ğŸ“· Uploaded Image", use_container_width=True)

    # Grounding DINO inference ìˆ˜í–‰
    image_source, image_tensor = load_image(uploaded_file)

    if apply_detection and class_labels:
        # ì‚¬ìš©ìê°€ ì…ë ¥í•œ í´ë˜ìŠ¤ ê¸°ë°˜ íƒìƒ‰ ìˆ˜í–‰
        text_prompt = ", ".join(class_labels)
        box_threshold = min(threshold_values.values()) if threshold_values else 0.5
        text_threshold = 0.25  # ê³ ì •

        # Object Detection ìˆ˜í–‰
        boxes, logits, phrases = predict(
            model=model,
            device=device,
            image=image_tensor,
            caption=text_prompt,
            box_threshold=box_threshold,
            text_threshold=text_threshold
        )

        # ê²°ê³¼ Annotate
        annotated_frame = annotate(image_source=image_source, boxes=boxes, logits=logits, phrases=phrases)
        
        annotated_frame = cv2.cvtColor(annotated_frame, cv2.COLOR_BGR2RGB)

        # ê²°ê³¼ í‘œì‹œ
        st.image(annotated_frame, caption="ğŸ“¸ Detected Objects", use_container_width=True)

        # íƒì§€ëœ ê°ì²´ ë¦¬ìŠ¤íŠ¸ ì¶œë ¥
        st.write("### ğŸ“‹ Detected Objects")
        for i, box in enumerate(boxes):
            label = phrases[i]
            confidence = logits[i]
            if confidence >= threshold_values.get(label, 0.5):  # ê° í´ë˜ìŠ¤ threshold ì ìš©
                st.write(f"**{label}** - Confidence: {confidence:.2f}")

    else:
        st.warning("Please enter at least one object class to detect.")

else:
    st.info("Upload an image to start detection.")